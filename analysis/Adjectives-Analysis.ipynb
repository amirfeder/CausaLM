{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adj_bias_aggressive_ratio_adj_1', 'adj_bias_gentle_ratio_adj_1', 'adj']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "all_experiments_dir = \"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Experiments/Sentiment/\"\n",
    "experiments = os.listdir(all_experiments_dir)\n",
    "experiments = [exp for exp in experiments if \"adj\" in exp]\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cace_results(all_experiments_dir, cur_experiment):\n",
    "    cur_experiment_dirs = os.listdir(os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/\")\n",
    "    sorted_versions = sorted([int(i.split('_')[1]) for i in cur_experiment_dirs])\n",
    "    cur_experiment_dir = os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/version_\" + str(sorted_versions[-2])\n",
    "    cur_treated_experiment_dir = os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/version_\" + str(sorted_versions[-1])\n",
    "#     print(cur_experiment_dir)\n",
    "    cur_experiment_dir_files = os.listdir(cur_experiment_dir)\n",
    "    f_file = glob.glob(cur_experiment_dir + \"/Sentiment_F_trained_F_ima_control_treated-test-predictions.csv\")\n",
    "    df_f = pd.read_csv(f_file[0])\n",
    "    cf_file = glob.glob(cur_experiment_dir + \"/Sentiment_CF_trained_F_ima_control_treated-test-predictions.csv\")\n",
    "    df_cf = pd.read_csv(cf_file[0])\n",
    "    task = \"ima\"\n",
    "    Task = task.capitalize()\n",
    "    a_f_file = sorted(glob.glob(cur_treated_experiment_dir + \"/Sentiment_F_trained_F_\" + task + \"_treated-test-predictions.csv\"))\n",
    "    df_a_f = pd.read_csv(a_f_file[0])\n",
    "    a_cf_file = sorted(glob.glob(cur_treated_experiment_dir + \"/Sentiment_CF_trained_F_\" + task + \"_treated-test-predictions.csv\"))\n",
    "    df_a_cf = pd.read_csv(a_cf_file[0])\n",
    "    \n",
    "    accs = [df_f.correct.sum()/len(df_f), df_cf.correct.sum()/len(df_cf), df_a_f.correct.sum()/len(df_a_f), df_a_cf.correct.sum()/len(df_a_cf)]\n",
    "    \n",
    "    class_cols = [i for i in df_f.columns if \"class\" in i]\n",
    "\n",
    "    TreATE_f = df_a_f[class_cols].subtract(df_f[class_cols]).abs().sum(axis=1).sum()/len(df_a_f)\n",
    "    TreATE_cf = df_a_cf[class_cols].subtract(df_f[class_cols]).abs().sum(axis=1).sum()/len(df_a_f)\n",
    "    ATE = df_f[class_cols].subtract(df_cf[class_cols]).abs().sum(axis=1).sum()/len(df_f)\n",
    "    TreATE = TreATE_f if abs(TreATE_f-ATE)<abs(TreATE_cf-ATE) else TreATE_cf\n",
    "\n",
    "    all_test_data = \"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Data/Sentiment/Raw/unified/\"\n",
    "    df_f = df_f.set_index([\"sample_index\"])\n",
    "    df_cf = df_cf.set_index([\"sample_index\"])\n",
    "    cur_test_data = os.path.join(all_test_data, cur_experiment + \"_test.csv\")\n",
    "    df_test_data = pd.read_csv(cur_test_data)\n",
    "    df_test_data = df_test_data.set_index([\"id\"])\n",
    "    mean_ratio_adj = df_test_data[\"ratio_adj\"].mean()\n",
    "    neg_concept_indices = df_test_data[df_test_data[\"ratio_adj\"] < mean_ratio_adj].index.tolist()\n",
    "    pos_concept_indices = df_test_data[df_test_data[\"ratio_adj\"] >= mean_ratio_adj].index.tolist()\n",
    "\n",
    "    tpr_gap_f = recall_score(df_f.loc[neg_concept_indices]['true'], df_f.loc[neg_concept_indices]['prediction'], average='weighted') - recall_score(df_f.loc[pos_concept_indices]['true'], df_f.loc[pos_concept_indices]['prediction'], average='weighted')\n",
    "    tpr_gap_cf = recall_score(df_cf.loc[neg_concept_indices]['true'], df_cf.loc[neg_concept_indices]['prediction'], average='weighted') - recall_score(df_cf.loc[pos_concept_indices]['true'], df_cf.loc[pos_concept_indices]['prediction'], average='weighted')    \n",
    "    \n",
    "    return ATE, TreATE, accs, np.absolute(tpr_gap_cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_accs(cur_df):\n",
    "    cur_df['sum_correct'] = cur_df['correct'].apply(lambda x: sum([int(i) for i in x.replace(' ', '').replace('[', '').replace(']', '').split(',')]))\n",
    "    cur_df['example_len'] = cur_df['correct'].apply(lambda x: len([int(i) for i in x.replace(' ', '').replace('[', '').replace(']', '').split(',')]))\n",
    "\n",
    "    return cur_df['sum_correct'].sum() / cur_df['example_len'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_treated_results(all_experiments_dir, cur_experiment):\n",
    "    task = \"ima\"\n",
    "    Task = task.upper()\n",
    "    cur_experiment_dirs = os.listdir(os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/\")\n",
    "    latest_version = str(sorted([int(i.split('_')[1]) for i in cur_experiment_dirs])[-4])\n",
    "    cur_experiment_dir = os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/version_\" + latest_version\n",
    "    cur_experiment_dir_files = os.listdir(cur_experiment_dir)\n",
    "    f_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + Task + \"_MLM_F_trained_F-test-predictions.csv\")\n",
    "    df_f = pd.read_csv(f_file[0])\n",
    "    cf_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + Task + \"_MLM_CF_trained_F-test-predictions.csv\")\n",
    "    df_cf = pd.read_csv(cf_file[0])\n",
    "    a_f_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + Task + '_' + task + \"_control_treated_F_trained_F-test-predictions.csv\")\n",
    "    df_a_f = pd.read_csv(a_f_file[0])\n",
    "    a_cf_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + Task + '_' + task + \"_control_treated_CF_trained_F-test-predictions.csv\")\n",
    "    df_a_cf = pd.read_csv(a_cf_file[0])\n",
    "    \n",
    "    treated_accs = [get_sequence_accs(df_f), get_sequence_accs(df_cf), get_sequence_accs(df_a_f), get_sequence_accs(df_a_cf)]\n",
    "    \n",
    "    return treated_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_control_results(all_experiments_dir, cur_experiment):\n",
    "    task = \"ima\"\n",
    "    control = \"POS_Tagging\"\n",
    "    cur_experiment_dirs = os.listdir(os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/\")\n",
    "    latest_version = str(sorted([int(i.split('_')[1]) for i in cur_experiment_dirs])[-4])\n",
    "    cur_experiment_dir = os.path.join(all_experiments_dir,cur_experiment) + \"/unified/COMPARE/lightning_logs/version_\" + latest_version\n",
    "    cur_experiment_dir_files = os.listdir(cur_experiment_dir)\n",
    "    f_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + control + \"_F_trained_F-test-predictions.csv\")\n",
    "    df_f = pd.read_csv(f_file[0])\n",
    "    cf_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + control + \"_CF_trained_F-test-predictions.csv\")\n",
    "    df_cf = pd.read_csv(cf_file[0])\n",
    "    a_f_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + control + '_' + task + \"_control_treated_F_trained_F-test-predictions.csv\")\n",
    "    df_a_f = pd.read_csv(a_f_file[0])\n",
    "    a_cf_file = glob.glob(cur_experiment_dir + \"/CONTROL_\" + control + '_' + task + \"_control_treated_CF_trained_F-test-predictions.csv\")\n",
    "    df_a_cf = pd.read_csv(a_cf_file[0])\n",
    "    \n",
    "    control_accs = [get_sequence_accs(df_f), get_sequence_accs(df_cf), get_sequence_accs(df_a_f), get_sequence_accs(df_a_cf)]\n",
    "    \n",
    "    return control_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirf/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 0.728890, TreATE: 0.637894 \n",
      "ATE: 0.381265, TreATE: 0.331363 \n",
      "ATE: 0.411683, TreATE: 0.391688 \n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments:\n",
    "    ATE, TreATE, accs, tpr_gap = get_cace_results(all_experiments_dir, experiment)\n",
    "#     print(experiment)\n",
    "    print(\"ATE: %3f, TreATE: %3f \" %(ATE, TreATE))\n",
    "#     print(\"TPR-GAP: \" + str(tpr_gap))\n",
    "#     print(\"Accuracies: \" + str(accs))\n",
    "\n",
    "#     treated_accs = get_treated_results(all_experiments_dir, experiment)\n",
    "#     control_accs = get_control_results(all_experiments_dir, experiment)\n",
    "\n",
    "#     print(\"Accuracies on Treated Concepts: \" + str(treated_accs))\n",
    "#     print(\"Accuracies on Control Concepts: \" + str(control_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1379492600422833\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"adj_bias_aggressive_ratio_adj_1\"\n",
    "# experiment_name = \"adj_bias_gentle_ratio_adj_1\"\n",
    "# experiment_name = \"adj\"\n",
    "\n",
    "all_experiments_dir = \"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Experiments/Sentiment/\"\n",
    "experiment_dirs = os.listdir(os.path.join(all_experiments_dir,experiment_name) + \"/unified/COMPARE/lightning_logs/\")\n",
    "sorted_versions = sorted([int(i.split('_')[1]) for i in experiment_dirs])\n",
    "experiment_dir = os.path.join(all_experiments_dir,experiment_name) + \"/unified/COMPARE/lightning_logs/version_\" + str(sorted_versions[-2])\n",
    "treated_experiment_dir = os.path.join(all_experiments_dir,experiment_name) + \"/unified/COMPARE/lightning_logs/version_\" + str(sorted_versions[-1])\n",
    "data_dir = \"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Data/Sentiment/Raw/unified/\"\n",
    "\n",
    "df_f = pd.read_csv(experiment_dir + \"/Sentiment_F_trained_F_ima_control_treated-test-predictions.csv\")\n",
    "df_cf = pd.read_csv(experiment_dir + \"/Sentiment_CF_trained_F_ima_control_treated-test-predictions.csv\")\n",
    "df_a_f = pd.read_csv(treated_experiment_dir + \"/Sentiment_F_trained_F_ima_treated-test-predictions.csv\")\n",
    "df_a_cf = pd.read_csv(treated_experiment_dir + \"/Sentiment_CF_trained_F_ima_treated-test-predictions.csv\")\n",
    "\n",
    "df_test_data = pd.read_csv(data_dir + experiment_name + \"_test.csv\")\n",
    "\n",
    "df_test_data = df_test_data.set_index([\"id\"])\n",
    "ratio_adj_med = df_test_data['ratio_adj'].median()\n",
    "print(ratio_adj_med)\n",
    "small_ratio_adj_ids = df_test_data[df_test_data[\"ratio_adj\"] < (ratio_adj_med*1.9)].index.tolist()\n",
    "print(len(small_ratio_adj_ids))\n",
    "\n",
    "df_f = df_f.set_index([\"sample_index\"])\n",
    "df_f = df_f.loc[small_ratio_adj_ids]\n",
    "df_cf = df_cf.set_index([\"sample_index\"])\n",
    "df_cf = df_cf.loc[small_ratio_adj_ids]\n",
    "df_a_f = df_a_f.set_index([\"sample_index\"])\n",
    "df_a_f = df_a_f.loc[small_ratio_adj_ids]\n",
    "df_a_cf = df_a_cf.set_index([\"sample_index\"])\n",
    "df_a_cf = df_a_cf.loc[small_ratio_adj_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7181130472703026, 0.6370708554357875)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cols = [i for i in df_f.columns if \"class\" in i]\n",
    "\n",
    "TreATE_f = df_a_f[class_cols].subtract(df_f[class_cols]).abs().sum(axis=1).sum()/len(df_a_f)\n",
    "TreATE_cf = df_a_cf[class_cols].subtract(df_f[class_cols]).abs().sum(axis=1).sum()/len(df_a_f)\n",
    "ATE = df_f[class_cols].subtract(df_cf[class_cols]).abs().sum(axis=1).sum()/len(df_f)\n",
    "TreATE = TreATE_f if abs(TreATE_f-ATE)<abs(TreATE_cf-ATE) else TreATE_cf\n",
    "\n",
    "ATE, TreATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938328499172999\n"
     ]
    }
   ],
   "source": [
    "df_f_control = pd.read_csv(\"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Experiments/Sentiment/adj_bias_aggressive_ratio_adj_1/unified/COMPARE/lightning_logs/version_4/CONTROL_IMA_F_trained_F-test-predictions.csv\")\n",
    "df_f_control['sum_correct'] = df_f_control['correct'].apply(lambda x: sum([int(i) for i in x.replace(' ', '').replace('[', '').replace(']', '').split(',')]))\n",
    "df_f_control['example_len'] = df_f_control['correct'].apply(lambda x: len([int(i) for i in x.replace(' ', '').replace('[', '').replace(']', '').split(',')]))\n",
    "# print(df_f_control.head())\n",
    "\n",
    "print(df_f_control['sum_correct'].sum() / df_f_control['example_len'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
