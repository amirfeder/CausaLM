{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ID                 Sentence  \\\n",
      "0  2018-En-mystery-05498      Alonzo feels angry.   \n",
      "1  2018-En-mystery-11722    Alonzo feels furious.   \n",
      "2  2018-En-mystery-11364  Alonzo feels irritated.   \n",
      "3  2018-En-mystery-14320    Alonzo feels enraged.   \n",
      "4  2018-En-mystery-14114    Alonzo feels annoyed.   \n",
      "\n",
      "                                 Template  Person Gender              Race  \\\n",
      "0  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
      "1  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
      "2  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
      "3  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
      "4  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
      "\n",
      "  Emotion Emotion word  \n",
      "0   anger        angry  \n",
      "1   anger      furious  \n",
      "2   anger    irritated  \n",
      "3   anger      enraged  \n",
      "4   anger      annoyed  \n"
     ]
    }
   ],
   "source": [
    "# from constants import POMS_GENDER_DATASETS_DIR, POMS_RAW_DATA_DIR, RANDOM_SEED\n",
    "# from datasets_utils import split_data, print_text_stats\n",
    "# from Timer import timer\n",
    "from tqdm.contrib.itertools import product\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "corpus_file = \"/home/amirf/GoogleDrive/AmirNadav/CausaLM/Data/POMS/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv\"\n",
    "output_file = corpus_file.replace(\".csv\", \"_enriched_full_noisy.csv\")\n",
    "\n",
    "df = pd.read_csv(corpus_file, header=0, encoding='utf-8')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'sadness', 'fear', 'joy', nan]\n",
      "['African-American', 'European', nan]\n",
      "['Alonzo', 'Jamel', 'Alphonse', 'Jerome', 'Leroy', 'Torrance', 'Darnell', 'Lamar', 'Malik', 'Terrence']\n",
      "['Adam', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Justin', 'Ryan', 'Andrew', 'Jack']\n",
      "['Nichelle', 'Shereen', 'Ebony', 'Latisha', 'Shaniqua', 'Jasmine', 'Tanisha', 'Tia', 'Lakisha', 'Latoya']\n",
      "['Amanda', 'Courtney', 'Heather', 'Melanie', 'Katie', 'Betsy', 'Kristin', 'Nancy', 'Stephanie', 'Ellen']\n",
      "['he', 'this man', 'this boy', 'my brother', 'my son', 'my husband', 'my boyfriend', 'my father', 'my uncle', 'my dad', 'him']\n",
      "['she', 'this woman', 'this girl', 'my sister', 'my daughter', 'my wife', 'my girlfriend', 'my mother', 'my aunt', 'my mom', 'her']\n",
      "anger: ['irritating' 'vexing' 'outrageous' 'annoying' 'displeasing']\n",
      "sadness: ['depressing' 'serious' 'grim' 'heartbreaking' 'gloomy']\n",
      "fear: ['horrible' 'threatening' 'terrifying' 'shocking' 'dreadful']\n",
      "joy: ['funny' 'hilarious' 'amazing' 'wonderful' 'great']\n",
      "nan: []\n"
     ]
    }
   ],
   "source": [
    "emotions = df['Emotion'].unique().tolist()\n",
    "print(emotions)\n",
    "\n",
    "races = df['Race'].unique().tolist()\n",
    "print(races)\n",
    "\n",
    "male_african = df[(df['Gender'] == 'male') & (df['Race'] == 'African-American')]['Person'].unique().tolist()\n",
    "print(male_african)\n",
    "\n",
    "male_european = df[(df['Gender'] == 'male') & (df['Race'] == 'European')]['Person'].unique().tolist()\n",
    "print(male_european)\n",
    "\n",
    "female_african = df[(df['Gender'] == 'female') & (df['Race'] == 'African-American')]['Person'].unique().tolist()\n",
    "print(female_african)\n",
    "\n",
    "female_european = df[(df['Gender'] == 'female') & (df['Race'] == 'European')]['Person'].unique().tolist()\n",
    "print(female_european)\n",
    "\n",
    "male = df[(df['Gender'] == 'male') & (df['Race'].isna())]['Person'].unique().tolist()\n",
    "print(male)\n",
    "\n",
    "female = df[(df['Gender'] == 'female') & (df['Race'].isna())]['Person'].unique().tolist()\n",
    "print(female)\n",
    "\n",
    "for emotion in emotions:\n",
    "    print(f\"{emotion}: {df[(df['Emotion'] == emotion) & (df['Template'].str.contains('emotional situation word'))]['Emotion word'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'sadness', 'fear', 'joy']\n",
      "['African-American', 'European']\n",
      "['this man', 'this boy', 'my brother', 'my son', 'my husband', 'my boyfriend', 'my father', 'my uncle', 'my dad']\n",
      "['this woman', 'this girl', 'my sister', 'my daughter', 'my wife', 'my girlfriend', 'my mother', 'my aunt', 'my mom']\n"
     ]
    }
   ],
   "source": [
    "emotions.pop()\n",
    "print(emotions)\n",
    "races.pop()\n",
    "print(races)\n",
    "male.pop()\n",
    "male.pop(0)\n",
    "print(male)\n",
    "female.pop()\n",
    "female.pop(0)\n",
    "print(female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<person subject> feels <emotion word>.'\n",
      " 'The situation makes <person object> feel <emotion word>.'\n",
      " 'I made <person object> feel <emotion word>.'\n",
      " '<person subject> made me feel <emotion word>.'\n",
      " 'I saw <person object> in the market.'\n",
      " 'I talked to <person object> yesterday.'\n",
      " '<person subject> goes to the school in our neighborhood.'\n",
      " '<person subject> has two children.'\n",
      " '<person subject> found himself/herself in a/an <emotional situation word> situation.'\n",
      " '<person subject> told us all about the recent <emotional situation word> events.'\n",
      " 'The conversation with <person object> was <emotional situation word>.']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(df['Template'].unique())\n",
    "print(len(df['Template'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': ['angry',\n",
       "  'furious',\n",
       "  'irritated',\n",
       "  'enraged',\n",
       "  'annoyed',\n",
       "  'irritating',\n",
       "  'vexing',\n",
       "  'outrageous',\n",
       "  'annoying',\n",
       "  'displeasing'],\n",
       " 'sadness': ['sad',\n",
       "  'depressed',\n",
       "  'devastated',\n",
       "  'miserable',\n",
       "  'disappointed',\n",
       "  'depressing',\n",
       "  'serious',\n",
       "  'grim',\n",
       "  'heartbreaking',\n",
       "  'gloomy'],\n",
       " 'fear': ['terrified',\n",
       "  'discouraged',\n",
       "  'scared',\n",
       "  'anxious',\n",
       "  'fearful',\n",
       "  'horrible',\n",
       "  'threatening',\n",
       "  'terrifying',\n",
       "  'shocking',\n",
       "  'dreadful'],\n",
       " 'joy': ['happy',\n",
       "  'ecstatic',\n",
       "  'glad',\n",
       "  'relieved',\n",
       "  'excited',\n",
       "  'funny',\n",
       "  'hilarious',\n",
       "  'amazing',\n",
       "  'wonderful',\n",
       "  'great']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict = {}\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_dict[emotion] = df[df['Emotion'] == emotion]['Emotion word'].unique().tolist()\n",
    "\n",
    "emotion_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = ['male', 'female']\n",
    "races = ['African-American', 'European', None]\n",
    "names = {\n",
    "    'male_': male,\n",
    "    'male_African-American' : male_african,\n",
    "    'male_European' : male_european,\n",
    "    'female_': female,\n",
    "    'female_African-American' : female_african,\n",
    "    'female_European' : female_european\n",
    "}\n",
    "places = ['bookstore', 'supermarket', 'market', 'shop', 'church',\n",
    "          'school', 'university', 'college', 'restaurant', 'hairdresser']\n",
    "seasons = ['winter', 'spring', 'summer', 'fall']\n",
    "\n",
    "times = ['all this time', 'all these years', 'these few days']\n",
    "\n",
    "family = ['siblings', 'children', 'kids', 'cousins']\n",
    "observe = ['saw', 'noticed', 'bumped into']\n",
    "numbers = ['no', 'one', 'two', 'three', 'four', 'five']\n",
    "days = ['yesterday', 'two days ago', 'last night', 'every day during the past month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 1 Sentences (Active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_words_dict = {\n",
    "    'anger': ['angry', 'furious', 'irritated', 'enraged', 'annoyed',\n",
    "             'irate', 'vexed', 'mad', 'infuriated', 'outraged'],\n",
    "    'sadness': ['sad', 'depressed', 'devastated', 'miserable', 'disappointed',\n",
    "               'unhappy', 'gloomy', 'crushed', 'downhearted', 'troubled'],\n",
    "    'fear': ['terrified', 'discouraged', 'scared', 'anxious','fearful',\n",
    "             'horrible', 'threatened', 'shocked', 'dreadful', 'frightened'],\n",
    "    'joy': ['happy', 'ecstatic', 'glad', 'relieved', 'excited',\n",
    "            'funny', 'amazed', 'wonderful', 'great', 'cheerful']\n",
    "}\n",
    "gender_nouns = { 'male': 'he', 'female': 'she'}\n",
    "sentences_dict = {\n",
    "    1: '<person> feels <emotion>',\n",
    "    2: 'The situation makes <person> feel <emotion>',\n",
    "    3: 'I made <person> feel <emotion>',\n",
    "    4: '<person> made me feel <emotion>',\n",
    "}\n",
    "\n",
    "# Enrich Existing sentences:\n",
    "sentence_prefixes = {\n",
    "    1: ['Now that it is all over, ',\n",
    "        'As <gender_noun> approaches the <place>, ',\n",
    "        'As <gender_noun> approaches the <place>, '],\n",
    "    2: ['While it is still under development, ',\n",
    "        'Even though it is still under development, ',\n",
    "        'While it is still under construction, ',\n",
    "        'Even though it is still a work in progress, ',\n",
    "        'While this is still under construction, ',\n",
    "        'There is still a long way to go, but '],\n",
    "    3: ['I have no idea how or why, but ',\n",
    "        'I do not know why, but ',\n",
    "        'It is a mystery to me, but it seems ',\n",
    "       'It is far from over, but so far '],\n",
    "    4: ['It was totally unexpected, but ',\n",
    "        'While we were at the <place>, ',\n",
    "        'We went to the <place>, and '],\n",
    "}\n",
    "\n",
    "sentence_suffixes = {\n",
    "    1: [' as <gender_noun> walks to the <place>',\n",
    "        ' as <gender_noun> paces along to the <place>',\n",
    "        ' at the end',\n",
    "        ' at the start'],\n",
    "    2: [', but it does not matter now',\n",
    "        ', and will probably continue to in the forseeable future'],\n",
    "    3: [', and plan to continue until the <season> is over',\n",
    "        ', time and time again'],\n",
    "    4: [' for the first time ever in my life',\n",
    "        ' whenever I came near'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Type 1 Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210e089786dc4f26bafbaac1c3aaa4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=240.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2412800\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    sentences_writer = csv.writer(csvfile, delimiter=',')\n",
    "    sentences_writer.writerow([\"ID\", \"Sentence\", \"Template\", \"Person\", \"Gender\", \"Race\", \"Emotion\", \"Emotion_word\"])\n",
    "\n",
    "with open(output_file, 'a', newline='') as csvfile:\n",
    "    sentences_writer = csv.writer(csvfile, delimiter=',')\n",
    "    for gender, race, place, season in product(genders, races, places, seasons):\n",
    "        for name in names[f\"{gender}_{race if race else ''}\"]:\n",
    "            for sentence_num, base_sentence in sentences_dict.items():\n",
    "                for cur_prefix in sentence_prefixes[sentence_num]:\n",
    "                    prefix_sentence = f\"{cur_prefix + base_sentence.lower().replace('<person>', name)}.\"\n",
    "                    prefix_sentence = prefix_sentence.replace('<place>', place).replace('<season>', season).replace('<gender_noun>', gender_nouns[gender])\n",
    "                    prefix_template = cur_prefix + base_sentence.lower()\n",
    "                    prefix_template = prefix_template.replace('<place>', place).replace('<season>', season)\n",
    "                    for emotion_label, emotion_words in emotion_words_dict.items():\n",
    "                        for word in emotion_words:\n",
    "                            cur_prefix_sentence = prefix_sentence.replace('<emotion>', word)\n",
    "                            sentences_writer.writerow([count, cur_prefix_sentence, prefix_template, name, gender, race, emotion_label, word])\n",
    "                            count += 1\n",
    "\n",
    "                for cur_suffix in sentence_suffixes[sentence_num]:    \n",
    "                    suffix_sentence = f\"{base_sentence.replace('<person>', name.capitalize() if base_sentence.startswith('<person>') else name)}{cur_suffix}.\"\n",
    "                    suffix_sentence = suffix_sentence.replace('<place>', place).replace('<season>', season).replace('<gender_noun>', gender_nouns[gender])\n",
    "                    suffix_template = base_sentence + cur_suffix\n",
    "                    suffix_template = suffix_template.replace('<place>', place).replace('<season>', season)\n",
    "                    for emotion_label, emotion_words in emotion_words_dict.items():\n",
    "                        for word in emotion_words:\n",
    "                            cur_suffix_sentence = suffix_sentence.replace('<emotion>', word)\n",
    "                            sentences_writer.writerow([count, cur_suffix_sentence, suffix_template, name, gender, race, emotion_label, word])\n",
    "                            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 2 Sentences (Passive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_words_dict = {\n",
    "    'anger': ['irritating', 'vexing', 'outrageous', 'annoying', 'displeasing'],\n",
    "    'sadness': ['depressing', 'serious', 'grim', 'heartbreaking', 'gloomy'],\n",
    "    'fear': ['horrible', 'threatening', 'terrifying', 'shocking', 'dreadful'],\n",
    "    'joy': ['funny', 'hilarious', 'amazing', 'wonderful', 'great']\n",
    "}\n",
    "\n",
    "gender_nouns = { 'male': 'himself', 'female': 'herself'}\n",
    "\n",
    "def get_indefinite(emotion):\n",
    "    return 'an' if emotion[0] in ['aeiou'] else 'a'\n",
    "\n",
    "sentences_dict = {\n",
    "    5: '<person> found <gender_noun> in <ind> <emotion> situation',\n",
    "    6: '<person> told us all about the recent <emotion> events',\n",
    "    7: 'The conversation with <person> was <emotion>',\n",
    "}\n",
    "\n",
    "# Enrich Existing sentences:\n",
    "sentence_prefixes = {\n",
    "    5: ['To our surprise, ',\n",
    "        'We were told that '],\n",
    "    6: ['While we were walking to the <place>, ',\n",
    "        'As we were walking together, '],\n",
    "    7: ['While unsurprising, ',\n",
    "        'As expected, ',\n",
    "        'To our amazement, ']\n",
    "}\n",
    "\n",
    "sentence_suffixes = {\n",
    "    5: [', after <time>',\n",
    "        ', something none of us expected'],\n",
    "    6: [' as we were walking to the <place>',\n",
    "        ', to our surprise'],\n",
    "    7: [', you could feel it in the air',\n",
    "        ', we could from simply looking']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Type 2 Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541b23f7737e431f92d0297cfaef6544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2865200\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'a', newline='') as csvfile:\n",
    "    sentences_writer = csv.writer(csvfile, delimiter=',')\n",
    "    for gender, race, place, time_word in product(genders, races, places, times):\n",
    "        for name in names[f\"{gender}_{race if race else ''}\"]:\n",
    "            for sentence_num, base_sentence in sentences_dict.items():\n",
    "                for cur_prefix in sentence_prefixes[sentence_num]:                    \n",
    "                    prefix_sentence = f\"{cur_prefix + base_sentence.lower().replace('<person>', name)}.\"\n",
    "                    prefix_sentence = prefix_sentence.replace('<place>', place).replace('<gender_noun>', gender_nouns[gender]).replace(\"<time>\", time_word)\n",
    "                    prefix_template = cur_prefix + base_sentence.lower()\n",
    "                    prefix_template = prefix_template.replace('<place>', place).replace('<time>', time_word)\n",
    "                    for emotion_label, emotion_words in emotion_words_dict.items():\n",
    "                        for word in emotion_words:\n",
    "                            cur_prefix_sentence = prefix_sentence.replace('<ind>', get_indefinite(emotion_label)).replace('<emotion>', word)\n",
    "                            sentences_writer.writerow([count, cur_prefix_sentence, prefix_template, name, gender, race, emotion_label, word])\n",
    "                            count += 1\n",
    "\n",
    "                for cur_suffix in sentence_suffixes[sentence_num]:                    \n",
    "                    suffix_sentence = f\"{base_sentence.replace('<person>', name.capitalize() if base_sentence.startswith('<person>') else name)}{cur_suffix}.\"\n",
    "                    suffix_sentence = suffix_sentence.replace('<place>', place).replace('<gender_noun>', gender_nouns[gender]).replace(\"<time>\", time_word)\n",
    "                    suffix_template = base_sentence + cur_suffix\n",
    "                    suffix_template = suffix_template.replace('<place>', place).replace('<time>', time_word)\n",
    "                    for emotion_label, emotion_words in emotion_words_dict.items():\n",
    "                        for word in emotion_words:\n",
    "                            cur_suffix_sentence = suffix_sentence.replace('<ind>', get_indefinite(emotion_label)).replace('<emotion>', word)\n",
    "                            sentences_writer.writerow([count, cur_suffix_sentence, suffix_template, name, gender, race, emotion_label, word])\n",
    "                            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 3 Sentences (No Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dict = {\n",
    "    8: 'I <observe> <person> in the <place> <day>.',\n",
    "    9: 'I talked to <person> <day>.',\n",
    "    10: '<person> goes to the school in our neighborhood.',\n",
    "    11: '<person> has <number> <family>.',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Type 3 Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d3eb8bdbc54293bd2a5da37c62913e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2880.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3581500\n"
     ]
    }
   ],
   "source": [
    "type3_count = 0\n",
    "with open(output_file, 'a', newline='') as csvfile:\n",
    "    sentences_writer = csv.writer(csvfile, delimiter=',')\n",
    "    for place, fam, obs, num, day in product(places, family, observe, numbers, days):\n",
    "        while type3_count < count // 4:\n",
    "            for sentence_num, base_sentence in sentences_dict.items():\n",
    "                for gender, race in itertools.product(genders, races):\n",
    "                    for name in names[f\"{gender}_{race if race else ''}\"]:\n",
    "                        if type3_count >= count // 4:\n",
    "                            break\n",
    "                        else:\n",
    "                            cur_sentence = base_sentence.replace('<person>', name.capitalize() if base_sentence.startswith('<person>') else name).replace('<place>', place).replace('<family>', fam).replace('<observe>', obs).replace('<number>', num).replace('<day>', day)\n",
    "                            template = base_sentence.replace('<place>', place).replace('<family>', fam).replace('<observe>', obs).replace('<number>', num).replace('<day>', day)\n",
    "                            sentences_writer.writerow([count, cur_sentence, template, name, gender, race, None, None])\n",
    "                            type3_count += 1\n",
    "print(count + type3_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Noise Additions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Correlated Noise Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise_sentences = [\"This is random noise\",\n",
    "                   \"This is only here to confuse the classifier\",\n",
    "                   \"No added information is given in this part\",\n",
    "                   \"Do not look here, it will just confuse you\",\n",
    "                   \"Sometimes noise helps, not here\",\n",
    "                   \"Really, there is no information here\",\n",
    "                   \"Nothing here is relevant\",\n",
    "                   \"This sentence is just a placeholder\",\n",
    "                   \"Why are you looking here\",\n",
    "                   \"When in doubt, use these words\",\n",
    "                   \"I'm just here so I won't get fined\",\n",
    "                   \"Yet another redundant sentence\",\n",
    "                   \"Look away, no information will be given here\",\n",
    "                  ]\n",
    "\n",
    "pdf_noisy_sentences_dict = {\n",
    "    1: [0.20]*3+[0.04]*10,\n",
    "    2: [0.04]*3+[0.20]*3+[0.04]*7,\n",
    "    3: [0.04]*6+[0.20]*3+[0.04]*4,\n",
    "    4: [0.04]*9+[0.20]*3+[0.04]*1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pseudo code for adding noise:\n",
    "\n",
    "for sentence in sentences:\n",
    "    label = sentence[\"Emotion\"]\n",
    "    if random.random() > 0.5: # Add noisy sentence w.p 0.5\n",
    "        noise_sentence_id = np.random.choice(13, 1, p=pdf_noisy_sentences_dict[1])[0] # Choose sentence according to pdf\n",
    "        if random.random() > 0.5: # Choose whether prefix or suffix\n",
    "            new_sentence += sentence + \". \" + noise_sentences[noise_sentence_id] + \".\"\n",
    "        else:\n",
    "            new_sentence = noise_sentences[noise_sentence_id] + \". \" + sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Emotion Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_emotion_words_dict = {\n",
    "    \"joy\": [\n",
    "        \"blissful\", \"joyous\", \"delighted\", \"overjoyed\", \"gleeful\", \"thankful\", \"festive\", \"ecstatic\", \"satisfied\", \"cheerful\",\n",
    "        \"sunny\", \"elated\", \"jubilant\", \"jovial\", \"lighthearted\", \"glorious\", \"innocent\", \"gratified\", \"euphoric\", \"world\", \n",
    "        \"playful\", \"courageous\", \"energetic\", \"liberated\", \"optimistic\", \"frisky\", \"animated\", \"spirited\", \"thrilled\",\n",
    "        \"intelligent\", \"exhilarated\", \"spunky\", \"youthful\", \"vigorous\", \"tickled\", \"creative\", \n",
    "        \"constructive\", \"helpful\", \"resourceful\", \"comfortable\", \"pleased\", \"encouraged\", \"surprised\", \"content\", \n",
    "        \"serene\", \"bright\", \"blessed\", \"Vibrant\", \"Bountiful\", \"Glowing\"\n",
    "    ],\n",
    "    \"anger\": [\n",
    "        \"Ordeal\", \"Outrageousness\", \"Provoke\", \"Repulsive\", \"Scandal\", \"Severe\", \"Shameful\", \"Shocking\", \"Terrible\", \"Tragic\",\n",
    "        \"Unreliable\", \"Unstable\", \"Wicked\", \"Aggravate\", \"Agony\", \"Appalled\", \"Atrocious\", \"Corrupting\", \"Damaging\",\n",
    "        \"Deplorable\", \"Disadvantages\", \"Disastrous\", \"Disgusted\", \"Dreadful\", \"Eliminate\", \"Harmful\", \"Harsh\", \"Inconsiderate\",\n",
    "        \"enraged\", \"offensive\", \"aggressive\", \"frustrated\", \"controlling\", \"resentful\", \"malicious\", \"infuriated\", \"critical\",\n",
    "        \"violent\", \"vindictive\", \"sadistic\", \"spiteful\", \"furious\", \"agitated\", \"antagonistic\", \"repulsed\", \"quarrelsome\", \n",
    "        \"venomous\", \"rebellious\", \"exasperated\", \"impatient\", \"contrary\", \"condemning\", \"seething\", \"scornful\", \"sarcastic\",\n",
    "        \"poisonous\", \"jealous\", \"revengeful\", \"retaliating\", \"reprimanding\", \"powerless\", \"despicable\", \"desperate\", \"alienated\", \n",
    "        \"pessimistic\", \"dejected\", \"vilified\", \"unjustified\", \"violated\"\n",
    "    ],\n",
    "    \"sadness\": [\n",
    "        \"bitter\", \"dismal\", \"heartbroken\", \"melancholy\", \"mournful\", \"pessimistic\", \"somber\", \"sorrowful\", \"sorry\", \"wistful\",\n",
    "        \"bereaved\", \"blue\", \"cheerless\", \"dejected\", \"despairing\", \"despondent\", \"disconsolate\", \"distressed\", \"doleful\", \n",
    "        \"down\", \"downcast\", \"forlorn\", \"glum\", \"grieved\", \"heartsick\", \"heavyhearted\", \"hurting\", \"languishing\", \n",
    "        \"low\", \"lugubrious\", \"morbid\", \"morose\", \"pensive\", \"troubled\", \"weeping\", \"woebegone\",\n",
    "    ],\n",
    "    \"fear\": [\n",
    "        \"angst\", \"anxiety\", \"concern\", \"despair\", \"dismay\", \"doubt\", \"dread\", \"horror\", \"jitters\", \"panic\", \"scare\", \n",
    "        \"suspicion\", \"terror\", \"unease\", \"uneasiness\", \"worry\", \"abhorrence\", \"agitation\", \"aversion\", \"awe\", \"consternation\",\n",
    "        \"cowardice\", \"creeps\", \"discomposure\", \"disquietude\", \"distress\", \"faintheartedness\", \"foreboding\", \"fright\", \"funk\",\n",
    "        \"misgiving\", \"nightmare\", \"phobia\", \"presentiment\", \"qualm\", \"reverence\", \"revulsion\", \"timidity\", \"trembling\",\n",
    "        \"tremor\", \"trepidation\", \"chickenheartedness\", \"recreancy\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3581500\n",
      "Index(['ID', 'Sentence', 'Template', 'Person', 'Gender', 'Race', 'Emotion',\n",
      "       'Emotion_word'],\n",
      "      dtype='object')\n",
      "fear       716300\n",
      "joy        716300\n",
      "sadness    716300\n",
      "anger      716300\n",
      "NaN        716300\n",
      "Name: Emotion, dtype: int64 \n",
      "\n",
      "female    1790750\n",
      "male      1790750\n",
      "Name: Gender, dtype: int64 \n",
      "\n",
      "African-American    1235000\n",
      "European            1235000\n",
      "NaN                 1111500\n",
      "Name: Race, dtype: int64 \n",
      "\n",
      "Ebony            61750\n",
      "my dad           61750\n",
      "Nichelle         61750\n",
      "Jerome           61750\n",
      "Latoya           61750\n",
      "Jack             61750\n",
      "Courtney         61750\n",
      "my mom           61750\n",
      "this girl        61750\n",
      "this man         61750\n",
      "Darnell          61750\n",
      "Lamar            61750\n",
      "Frank            61750\n",
      "Jamel            61750\n",
      "Alonzo           61750\n",
      "my sister        61750\n",
      "this boy         61750\n",
      "Stephanie        61750\n",
      "my uncle         61750\n",
      "Kristin          61750\n",
      "Leroy            61750\n",
      "Roger            61750\n",
      "my boyfriend     61750\n",
      "my brother       61750\n",
      "Jasmine          61750\n",
      "Shereen          61750\n",
      "my wife          61750\n",
      "Lakisha          61750\n",
      "Malik            61750\n",
      "Adam             61750\n",
      "Josh             61750\n",
      "Harry            61750\n",
      "Ryan             61750\n",
      "Betsy            61750\n",
      "Tia              61750\n",
      "my father        61750\n",
      "my girlfriend    61750\n",
      "Tanisha          61750\n",
      "Torrance         61750\n",
      "my daughter      61750\n",
      "Amanda           61750\n",
      "Justin           61750\n",
      "Katie            61750\n",
      "Ellen            61750\n",
      "my husband       61750\n",
      "Terrence         61750\n",
      "Alphonse         61750\n",
      "Nancy            61750\n",
      "Melanie          61750\n",
      "this woman       61750\n",
      "Heather          61750\n",
      "Shaniqua         61750\n",
      "Andrew           61750\n",
      "my mother        61750\n",
      "my aunt          61750\n",
      "Alan             61750\n",
      "Latisha          61750\n",
      "my son           61750\n",
      "Name: Person, dtype: int64 \n",
      "\n",
      "NaN              716300\n",
      "horrible          82940\n",
      "great             82940\n",
      "funny             82940\n",
      "dreadful          82940\n",
      "gloomy            82940\n",
      "wonderful         82940\n",
      "devastated        60320\n",
      "excited           60320\n",
      "scared            60320\n",
      "depressed         60320\n",
      "terrified         60320\n",
      "furious           60320\n",
      "glad              60320\n",
      "infuriated        60320\n",
      "annoyed           60320\n",
      "fearful           60320\n",
      "discouraged       60320\n",
      "threatened        60320\n",
      "vexed             60320\n",
      "mad               60320\n",
      "enraged           60320\n",
      "outraged          60320\n",
      "amazed            60320\n",
      "disappointed      60320\n",
      "downhearted       60320\n",
      "irritated         60320\n",
      "miserable         60320\n",
      "ecstatic          60320\n",
      "crushed           60320\n",
      "happy             60320\n",
      "irate             60320\n",
      "angry             60320\n",
      "sad               60320\n",
      "unhappy           60320\n",
      "frightened        60320\n",
      "shocked           60320\n",
      "anxious           60320\n",
      "cheerful          60320\n",
      "relieved          60320\n",
      "troubled          60320\n",
      "shocking          22620\n",
      "annoying          22620\n",
      "vexing            22620\n",
      "displeasing       22620\n",
      "terrifying        22620\n",
      "amazing           22620\n",
      "hilarious         22620\n",
      "threatening       22620\n",
      "heartbreaking     22620\n",
      "outrageous        22620\n",
      "depressing        22620\n",
      "irritating        22620\n",
      "serious           22620\n",
      "grim              22620\n",
      "Name: Emotion_word, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(enriched_df))\n",
    "print(enriched_df.columns)\n",
    "print(enriched_df[\"Emotion\"].value_counts(dropna=False),\"\\n\")\n",
    "print(enriched_df[\"Gender\"].value_counts(dropna=False),\"\\n\")\n",
    "print(enriched_df[\"Race\"].value_counts(dropna=False),\"\\n\")\n",
    "print(enriched_df[\"Person\"].value_counts(dropna=False),\"\\n\")\n",
    "print(enriched_df[\"Emotion_word\"].value_counts(dropna=False),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadavo/anaconda3/envs/causalm/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                          Sentence  \\\n",
      "0   0      Now that it is all over, Alonzo feels angry.   \n",
      "1   1    Now that it is all over, Alonzo feels furious.   \n",
      "2   2  Now that it is all over, Alonzo feels irritated.   \n",
      "3   3    Now that it is all over, Alonzo feels enraged.   \n",
      "4   4    Now that it is all over, Alonzo feels annoyed.   \n",
      "\n",
      "                                            Template  Person Gender  \\\n",
      "0  Now that it is all over, <person> feels <emotion>  Alonzo   male   \n",
      "1  Now that it is all over, <person> feels <emotion>  Alonzo   male   \n",
      "2  Now that it is all over, <person> feels <emotion>  Alonzo   male   \n",
      "3  Now that it is all over, <person> feels <emotion>  Alonzo   male   \n",
      "4  Now that it is all over, <person> feels <emotion>  Alonzo   male   \n",
      "\n",
      "               Race Emotion Emotion_word  \n",
      "0  African-American   anger        angry  \n",
      "1  African-American   anger      furious  \n",
      "2  African-American   anger    irritated  \n",
      "3  African-American   anger      enraged  \n",
      "4  African-American   anger      annoyed  \n",
      "              ID                                           Sentence  \\\n",
      "2918255  2865200      Katie goes to the school in our neighborhood.   \n",
      "3556901  2865200                   I talked to my sister yesterday.   \n",
      "2089207  2089207  While we were at the supermarket, my daughter ...   \n",
      "2326865  2326865  While it is still under construction, the situ...   \n",
      "3035948  2865200                     My girlfriend has no siblings.   \n",
      "\n",
      "                                                  Template         Person  \\\n",
      "2918255   <person> goes to the school in our neighborhood.          Katie   \n",
      "3556901                    I talked to <person> yesterday.      my sister   \n",
      "2089207  While we were at the supermarket, <person> mad...    my daughter   \n",
      "2326865  While it is still under construction, the situ...        my aunt   \n",
      "3035948                          <person> has no siblings.  my girlfriend   \n",
      "\n",
      "         Gender      Race Emotion Emotion_word  \n",
      "2918255  female  European     NaN          NaN  \n",
      "3556901  female       NaN     NaN          NaN  \n",
      "2089207  female       NaN   anger          mad  \n",
      "2326865  female       NaN    fear     horrible  \n",
      "3035948  female       NaN     NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "enriched_df = pd.read_csv(output_file, header=0)\n",
    "print(enriched_df.head())\n",
    "shuffled_enriched_df = enriched_df.sample(frac=1)\n",
    "print(shuffled_enriched_df.head())\n",
    "shuffled_enriched_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
